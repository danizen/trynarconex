<?xml version="1.0" encoding="UTF-8"?>
<!--
   Copyright 2010-2016 Norconex Inc.

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->
<!-- This configuration shows is an integrated crawler for cancer.org -->
<httpcollector id="MedlinePlus Test HTTP Collector">

  <!-- Decide where to store generated files. -->
  <progressDir>./output/medlineplus/progress</progressDir>
  <logsDir>./output/medlineplus/logs</logsDir>

  <crawlers>
    <crawler id="Norconex MedlinePlus Test">

      <userAgent>dan crawl for MedlinePlus</userAgent>

      <!-- Requires at least one start URL (or urlsFile).
           Optionally limit crawling to same protocol/domain/port as
           start URLs. -->
      <startURLs stayOnDomain="true" stayOnPort="true" stayOnProtocol="true">
        <url>https://www.cancer.org/cancer/acute-lymphocytic-leukemia/causes-risks-prevention/risk-factors.html</url>
      </startURLs>

      <!-- === Recommendations: ============================================ -->

      <!-- Specify a crawler default directory where to generate files. -->
      <workDir>./output/medlineplus</workDir>

      <numThreads>5</numThreads>

      <!-- Put a maximum depth to avoid infinite crawling (e.g. calendars). -->
      <maxDepth>5</maxDepth>

      <keepDownloads>true</keepDownloads>

      <maxDocuments>40</maxDocuments>

      <!-- We know we don't want to crawl the entire site, so ignore sitemap. -->
      <sitemapResolverFactory ignore="true" />

      <referenceFilters>
        <filter class="com.norconex.collector.core.filter.impl.ExtensionReferenceFilter" onMatch="exclude">
          jpg,gif,png,ico,css,js
        </filter>
      </referenceFilters>

      <!-- Be as nice as you can to sites you crawl. -->
      <delay default="1000" />

      <!-- Document importing -->
      <importer>
        <postParseHandlers>
          <tagger class="com.norconex.importer.handler.tagger.impl.RenameTagger">
            <rename fromField="document.reference" toField="url" overwrite="true"/>
          </tagger>
          <tagger class="com.norconex.importer.handler.tagger.impl.SplitTagger">
            <split fromField="keywords" regex="true">
              <separator>\s*,\s*</separator>
            </split>
          </tagger>
          <tagger class="com.norconex.importer.handler.tagger.impl.CurrentDateTagger"
                  field="crawled"
                  format="yyyy-MM-dd HH:mm:ss"/>
          <tagger class="net.danizen.UUIDTagger"
                  field="uuid" overrite="true"/>
          <tagger class="com.norconex.importer.handler.tagger.impl.KeepOnlyTagger">
            <fields>title,keywords,description,url,uuid,md5sum,crawled</fields>
          </tagger>
        </postParseHandlers>
      </importer>

      <documentCheckSummer keep="true" targetField="md5sum"/>

      <!-- Decide what to do with your files by specifying a Committer. -->
      <committer class="com.norconex.committer.elasticsearch.ElasticsearchCommitter">
        <sourceReferenceField keep="false">uuid</sourceReferenceField>
        <indexName>medlineplus</indexName>
        <typeName>page</typeName>
        <queueDir>output/medlineplus/queue</queueDir>
        <queueSize>20</queueSize>
        <commitSize>10</commitSize>
      </committer>

    </crawler>
  </crawlers>

</httpcollector>
