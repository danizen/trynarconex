<?xml version="1.0" encoding="UTF-8"?>
<!--
   Copyright 2010-2016 Norconex Inc.

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->
<!-- This configuration shows is an integrated crawler for cancer.org -->
<httpcollector id="MedlinePlus Test HTTP Collector">

  <!-- Decide where to store generated files. -->
  <progressDir>$workdir/progress</progressDir>
  <logsDir>$workdir/logs</logsDir>

  <crawlers>
    <crawler id="MedlinePlus Monitor">

      <userAgent>$useragent</userAgent>

      <!-- Requires at least one start URL (or urlsFile).
           Optionally limit crawling to same protocol/domain/port as
           start URLs. -->
      <startURLs stayOnDomain="true" stayOnPort="true" stayOnProtocol="true">
        <url>$starturl</url>
      </startURLs>

      <!-- === Recommendations: ============================================ -->

      <!-- Specify a crawler default directory where to generate files. -->
      <workDir>$workdir</workDir>

      <numThreads>5</numThreads>

      <!-- Put a maximum depth to avoid infinite crawling (e.g. calendars). -->
      <maxDepth>$maxdepth</maxDepth>

      <maxDocuments>$maxdocs</maxDocuments>

      <!-- We know we don't want to crawl the entire site, so ignore sitemap. -->
      <sitemapResolverFactory ignore="true" />

      <!--
      <crawlDataStoreFactory class="com.norconex.collector.http.data.store.impl.mongo.MongoCrawlDataStoreFactory">
        <host>$mongo_host</host>
        <dbname>$mongo_dbname</dbname>
        <username>$mongo_username</username>
        <password>$mongo_password</password>
      </crawlDataStoreFactory>
      -->

      <referenceFilters>
        <filter class="com.norconex.collector.core.filter.impl.ExtensionReferenceFilter" onMatch="exclude">
          jpg,gif,png,ico,css,js,svg
        </filter>
        <filter class="com.norconex.collector.core.filter.impl.RegexReferenceFilter">https://[^/]+/news/.*</filter>
      </referenceFilters>

      <!-- Be as nice as you can to sites you crawl. -->
      <delay default="1000" />

      <!-- Document importing -->
      <importer>
        <postParseHandlers>
          <transformer class="com.norconex.importer.handler.transformer.impl.ReplaceTransformer">
            <replace>
              <fromValue>[\r\n\t\s]+</fromValue>
              <toValue xml:space="preserve"> </toValue>
            </replace>
          </transformer>
          <tagger class="com.norconex.importer.handler.tagger.impl.LanguageTagger">
            <languages>en, es</languages>
          </tagger>
          <tagger class="com.norconex.importer.handler.tagger.impl.RenameTagger">
            <rename fromField="document.reference" toField="url" overwrite="true"/>
            <rename fromField="document.language" toField="language"/>
          </tagger>
          <tagger class="com.norconex.importer.handler.tagger.impl.SplitTagger">
            <split fromField="keywords" regex="true">
              <separator>\s*,\s*</separator>
            </split>
          </tagger>
          <tagger class="com.norconex.importer.handler.tagger.impl.ScriptTagger">
            <script><![CDATA[
              var expr = new RegExp('[a-z]+://([^/]+).*');
              var url = metadata.url[0];
              var domain = url.replace(expr, '$1');
              metadata.addString('domain', domain);
            ]]></script>
          </tagger>
          <tagger class="com.norconex.importer.handler.tagger.impl.ConstantTagger" onConflict="noop">
            <constant name="status">new</constant>
          </tagger>
          <tagger class="com.norconex.importer.handler.tagger.impl.CurrentDateTagger"
                  field="crawled"
                  format="yyyy-MM-dd HH:mm:ss"/>
          <tagger class="net.danizen.UUIDTagger" field="uuid" overwrite="true"/>
          <tagger class="com.norconex.importer.handler.tagger.impl.KeepOnlyTagger">
            <fields>title,keywords,description,url,uuid,md5sum,crawled,domain,status,language</fields>
            <!-- <fieldsRegex>DC.*</fieldsRegex> -->
          </tagger>
        </postParseHandlers>
      </importer>

      <documentChecksummer keep="true" targetField="md5sum"/>

      <!-- Decide what to do with your files by specifying a Committer. -->
      <committer class="net.danizen.norconex.committer.ElasticsearchCommitter">
        <serverUrl>$es_server</serverUrl>
        <username>$es_username</username>
        <password>$es_password</password>
        <indexName>$es_indexname</indexName>
        <typeName>$es_typename</typeName>
        <sourceReferenceField keep="false">uuid</sourceReferenceField>
        <queueDir>$workdir/queue</queueDir>
        <queueSize>$es_queuesize</queueSize>
        <commitSize>$es_commitsize</commitSize>
      </committer>

      <!--
      <committer class="com.norconex.committer.core.impl.FileSystemCommitter">
        <directory>$workdir/files</directory>
      </committer>
      -->

    </crawler>
  </crawlers>

</httpcollector>
